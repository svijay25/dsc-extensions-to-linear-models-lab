{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Linear Models - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice many concepts you have learned so far, from adding interactions and polynomials to your model to regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Build a linear regression model with interactions and polynomial features \n",
    "- Use feature selection to obtain the optimal subset of features in a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Get Started!\n",
    "\n",
    "Below we import all the necessary packages for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\"ames.csv\")\n",
    "# Subset columns\n",
    "df = df[['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF',\n",
    "         '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd',\n",
    "         'GarageArea', 'Fireplaces', 'SalePrice']]\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df['SalePrice']\n",
    "X = df.drop(columns='SalePrice')\n",
    "\n",
    "# Split into train, test, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Housing Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we imported the Ames housing data and grabbed a subset of the data to use in this analysis.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "- Scale all the predictors using `StandardScaler`, then convert these scaled features back into DataFrame objects\n",
    "- Build a baseline `LinearRegression` model using *scaled variables* as predictors and use the $R^2$ score to evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Linear Regression Model:\n",
      "Train R²: 0.7868344817421309\n",
      "Train MSE: 1254529861.820535\n",
      "Validation R²: 0.6375622643038106\n",
      "Validation MSE: 2398973372.1732197\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Scale X_train and X_val using StandardScaler\n",
    "\n",
    "# Ensure X_train and X_val are scaled DataFrames\n",
    "# (hint: you can set the columns using X.columns)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and validation data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Convert the scaled arrays back into DataFrame objects\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)\n",
    "\n",
    "\n",
    "# Initialize the LinearRegression model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Fit the model on the scaled training data\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "train_r2 = linreg.score(X_train_scaled, y_train)\n",
    "train_mse = mean_squared_error(y_train, linreg.predict(X_train_scaled))\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_r2 = linreg.score(X_val_scaled, y_val)\n",
    "val_mse = mean_squared_error(y_val, linreg.predict(X_val_scaled))\n",
    "\n",
    "print('Baseline Linear Regression Model:')\n",
    "print('Train R²:', train_r2)\n",
    "print('Train MSE:', train_mse)\n",
    "print('Validation R²:', val_r2)\n",
    "print('Validation MSE:', val_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Interactions\n",
    "\n",
    "Instead of adding all possible interaction terms, let's try a custom technique. We are only going to add the interaction terms that increase the $R^2$ score as much as possible. Specifically we are going to look for the 7 interaction terms that each cause the most increase in the coefficient of determination.\n",
    "\n",
    "### Find the Best Interactions\n",
    "\n",
    "Look at all the possible combinations of variables for interactions by adding interactions one by one to the baseline model. Create a data structure that stores the pair of columns used as well as the $R^2$ score for each combination.\n",
    "\n",
    "***Hint:*** We have imported the `combinations` function from `itertools` for you ([documentation here](https://docs.python.org/3/library/itertools.html#itertools.combinations)). Try applying this to the columns of `X_train` to find all of the possible pairs.\n",
    "\n",
    "Print the 7 interactions that result in the highest $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction: ('LotArea', '1stFlrSF'), R²: 0.7211105666140571\n",
      "Interaction: ('LotArea', 'TotalBsmtSF'), R²: 0.707164920705011\n",
      "Interaction: ('LotArea', 'GrLivArea'), R²: 0.6690980823779022\n",
      "Interaction: ('LotArea', 'Fireplaces'), R²: 0.6529699515652586\n",
      "Interaction: ('2ndFlrSF', 'TotRmsAbvGrd'), R²: 0.647299489040519\n",
      "Interaction: ('OverallCond', 'TotalBsmtSF'), R²: 0.642901987923377\n",
      "Interaction: ('OverallQual', '2ndFlrSF'), R²: 0.6422324294284367\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the results\n",
    "interaction_results = []\n",
    "\n",
    "# Generate all possible pairs of columns from X_train\n",
    "column_pairs = list(combinations(X_train.columns, 2))\n",
    "\n",
    "for pair in column_pairs:\n",
    "    # Make copies of X_train and X_val\n",
    "    X_train_copy = X_train_scaled.copy()\n",
    "    X_val_copy = X_val_scaled.copy()\n",
    "    \n",
    "    # Create the interaction term\n",
    "    interaction_term_train = X_train_copy[pair[0]] * X_train_copy[pair[1]]\n",
    "    interaction_term_val = X_val_copy[pair[0]] * X_val_copy[pair[1]]\n",
    "    \n",
    "    # Add the interaction term to the dataframes\n",
    "    interaction_term_name = f\"{pair[0]}_x_{pair[1]}\"\n",
    "    X_train_copy[interaction_term_name] = interaction_term_train\n",
    "    X_val_copy[interaction_term_name] = interaction_term_val\n",
    "    \n",
    "    # Fit a linear regression model\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train_copy, y_train)\n",
    "    \n",
    "    # Evaluate the model on the validation data\n",
    "    val_r2 = linreg.score(X_val_copy, y_val)\n",
    "    \n",
    "    # Append the results to the data structure\n",
    "    interaction_results.append((pair, val_r2))\n",
    "\n",
    "# Sort the results by the R^2 score in descending order\n",
    "interaction_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 7 interactions\n",
    "top_7_interactions = interaction_results[:7]\n",
    "\n",
    "# Print the top 7 interactions\n",
    "for interaction in top_7_interactions:\n",
    "    print(f\"Interaction: {interaction[0]}, R²: {interaction[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Best Interactions\n",
    "\n",
    "Write code to include the 7 most important interactions in `X_train` and `X_val` by adding 7 columns. Use the naming convention `\"col1_col2\"`, where `col1` and `col2` are the two columns in the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over top 7 interactions\n",
    "for interaction in top_7_interactions:\n",
    "    # Extract column names from data structure\n",
    "    col1, col2 = interaction[0]\n",
    "    \n",
    "    # Construct new column name\n",
    "    interaction_term_name = f\"{col1}_x_{col2}\"\n",
    "    \n",
    "    # Add new column to X_train and X_val\n",
    "    X_train_scaled[interaction_term_name] = X_train_scaled[col1] * X_train_scaled[col2]\n",
    "    X_val_scaled[interaction_term_name] = X_val_scaled[col1] * X_val_scaled[col2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Polynomials\n",
    "\n",
    "Now let's repeat that process for adding polynomial terms.\n",
    "\n",
    "### Find the Best Polynomials\n",
    "\n",
    "Try polynomials of degrees 2, 3, and 4 for each variable, in a similar way you did for interactions (by looking at your baseline model and seeing how $R^2$ increases). Do understand that when going for a polynomial of degree 4 with `PolynomialFeatures`, the particular column is raised to the power of 2 and 3 as well in other terms.\n",
    "\n",
    "We only want to include \"pure\" polynomials, so make sure no interactions are included.\n",
    "\n",
    "Once again you should make a data structure that contains the values you have tested. We recommend a list of tuples of the form:\n",
    "\n",
    "`(col_name, degree, R2)`, so eg. `('OverallQual', 2, 0.781)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: GrLivArea, Degree: 3, R²: 0.8283344365591123\n",
      "Column: OverallQual, Degree: 3, R²: 0.8068023565969221\n",
      "Column: OverallQual, Degree: 4, R²: 0.8033455378866852\n",
      "Column: OverallQual, Degree: 2, R²: 0.8025833373594087\n",
      "Column: LotArea, Degree: 4, R²: 0.8003087859249637\n",
      "Column: 2ndFlrSF, Degree: 3, R²: 0.776751424068592\n",
      "Column: 2ndFlrSF, Degree: 4, R²: 0.7696907490848108\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Set up data structure\n",
    "\n",
    "# Initialize a list to store the results\n",
    "polynomial_results = []\n",
    "\n",
    "# Loop over all columns\n",
    "for col in X_train.columns:\n",
    "    # Loop over degrees 2, 3, 4\n",
    "    for degree in [2, 3, 4]:\n",
    "        # Make a copy of X_train and X_val\n",
    "        X_train_copy = X_train_scaled.copy()\n",
    "        X_val_copy = X_val_scaled.copy()\n",
    "        \n",
    "        # Instantiate PolynomialFeatures with relevant degree\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        \n",
    "        # Fit polynomial to column and transform column\n",
    "        poly_train = poly.fit_transform(X_train_copy[[col]])\n",
    "        poly_val = poly.transform(X_val_copy[[col]])\n",
    "        \n",
    "        # Convert the result to a DataFrame\n",
    "        poly_train_df = pd.DataFrame(poly_train, columns=[f\"{col}^d\" for d in range(1, degree + 1)])\n",
    "        poly_val_df = pd.DataFrame(poly_val, columns=[f\"{col}^d\" for d in range(1, degree + 1)])\n",
    "        \n",
    "        # Drop the original column before combining\n",
    "        X_train_copy = X_train_copy.drop(columns=[col])\n",
    "        X_val_copy = X_val_copy.drop(columns=[col])\n",
    "        \n",
    "        # Add polynomial to data\n",
    "        X_train_copy = pd.concat([X_train_copy, poly_train_df], axis=1)\n",
    "        X_val_copy = pd.concat([X_val_copy, poly_val_df], axis=1)\n",
    "        \n",
    "        # Fit a linear regression model\n",
    "        linreg = LinearRegression()\n",
    "        linreg.fit(X_train_copy, y_train)\n",
    "        \n",
    "        # Evaluate the model on the validation data\n",
    "        val_r2 = linreg.score(X_val_copy, y_val)\n",
    "        \n",
    "        # Append to data structure\n",
    "        polynomial_results.append((col, degree, val_r2))\n",
    "\n",
    "# Sort the results by the R^2 score in descending order\n",
    "polynomial_results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Get the top 7 polynomial terms\n",
    "top_7_polynomials = polynomial_results[:7]\n",
    "\n",
    "# Print the top 7 polynomial terms\n",
    "for poly in top_7_polynomials:\n",
    "    print(f\"Column: {poly[0]}, Degree: {poly[1]}, R²: {poly[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Best Polynomials\n",
    "\n",
    "If there are duplicate column values in the results above, don't add multiple of them to the model, to avoid creating duplicate columns. (For example, if column `A` appeared in your list as both a 2nd and 3rd degree polynomial, adding both would result in `A` squared being added to the features twice.) Just add in the polynomial that results in the highest R-Squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the best polynomial term for each column\n",
    "best_polynomials = {}\n",
    "\n",
    "# Loop over the top 7 polynomial terms\n",
    "for poly in top_7_polynomials:\n",
    "    col, degree, r2 = poly\n",
    "    # If the column is not already in the dictionary, or if the current polynomial term has a higher R² score\n",
    "    if col not in best_polynomials or r2 > best_polynomials[col][1]:\n",
    "        best_polynomials[col] = (degree, r2)\n",
    "\n",
    "# Loop over the best polynomial terms\n",
    "for col, (degree, r2) in best_polynomials.items():\n",
    "    # Make a copy of X_train and X_val\n",
    "    X_train_copy = X_train_scaled.copy()\n",
    "    X_val_copy = X_val_scaled.copy()\n",
    "    \n",
    "    # Instantiate PolynomialFeatures with relevant degree\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    # Fit polynomial to column and transform column\n",
    "    poly_train = poly.fit_transform(X_train_copy[[col]])\n",
    "    poly_val = poly.transform(X_val_copy[[col]])\n",
    "    \n",
    "    # Convert the result to a DataFrame\n",
    "    poly_train_df = pd.DataFrame(poly_train, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "    poly_val_df = pd.DataFrame(poly_val, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "    \n",
    "    # Drop the original column before combining\n",
    "    X_train_scaled = X_train_scaled.drop(columns=[col])\n",
    "    X_val_scaled = X_val_scaled.drop(columns=[col])\n",
    "    \n",
    "    # Add polynomial to data\n",
    "    X_train_scaled = pd.concat([X_train_scaled, poly_train_df], axis=1)\n",
    "    X_val_scaled = pd.concat([X_val_scaled, poly_val_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your final data set and make sure that your interaction terms as well as your polynomial terms are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OverallCond  TotalBsmtSF  1stFlrSF  TotRmsAbvGrd  GarageArea  Fireplaces  \\\n",
      "0    -0.509252    -0.639316 -0.804789      0.250689    0.327629   -0.994820   \n",
      "1    -0.509252     0.838208  0.641608     -0.365525    0.079146   -0.994820   \n",
      "2     1.304613    -0.012560 -0.329000     -0.981739   -1.105931   -0.994820   \n",
      "3     1.304613    -0.339045 -0.609036     -0.981739   -1.134602    0.588023   \n",
      "4     1.304613    -2.531499 -1.315922      0.250689   -2.281450   -0.994820   \n",
      "\n",
      "   LotArea_x_1stFlrSF  LotArea_x_TotalBsmtSF  LotArea_x_GrLivArea  \\\n",
      "0            0.092318               0.073336            -0.057254   \n",
      "1           -0.113385              -0.148128             0.043694   \n",
      "2            0.081045               0.003094             0.232730   \n",
      "3            0.230591               0.128368             0.433899   \n",
      "4            0.014341               0.027589             0.005250   \n",
      "\n",
      "   LotArea_x_Fireplaces  ...  OverallQual^1  OverallQual^2  OverallQual^3  \\\n",
      "0              0.114116  ...      -0.099842       0.009968      -0.000995   \n",
      "1              0.175804  ...       0.632038       0.399472       0.252481   \n",
      "2              0.245060  ...      -0.831723       0.691762      -0.575354   \n",
      "3             -0.222636  ...      -0.831723       0.691762      -0.575354   \n",
      "4              0.010842  ...      -1.563603       2.444854      -3.822780   \n",
      "\n",
      "   LotArea^1  LotArea^2  LotArea^3     LotArea^4  2ndFlrSF^1  2ndFlrSF^2  \\\n",
      "0  -0.114710   0.013158  -0.001509  1.731453e-04    1.261552    1.591512   \n",
      "1  -0.176719   0.031230  -0.005519  9.752976e-04   -0.808132    0.653077   \n",
      "2  -0.246336   0.060682  -0.014948  3.682263e-03   -0.808132    0.653077   \n",
      "3  -0.378617   0.143351  -0.054275  2.054946e-02   -0.808132    0.653077   \n",
      "4  -0.010898   0.000119  -0.000001  1.410729e-08    0.550523    0.303075   \n",
      "\n",
      "   2ndFlrSF^3  \n",
      "0    2.007775  \n",
      "1   -0.527772  \n",
      "2   -0.527772  \n",
      "3   -0.527772  \n",
      "4    0.166850  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "   OverallCond  TotalBsmtSF  1stFlrSF  TotRmsAbvGrd  GarageArea  Fireplaces  \\\n",
      "0     0.397681    -0.248487 -0.598161     -0.981739   -0.178895   -0.994820   \n",
      "1    -0.509252    -0.205591 -0.549222      0.250689   -0.465607   -0.994820   \n",
      "2     0.397681    -0.639316 -1.044043     -0.365525   -0.427379   -0.994820   \n",
      "3    -3.230050     1.891539  1.843314      0.866903    1.703847    0.588023   \n",
      "4    -0.509252    -1.616388 -0.995104      0.250689   -0.408265    0.588023   \n",
      "\n",
      "   LotArea_x_1stFlrSF  LotArea_x_TotalBsmtSF  LotArea_x_GrLivArea  \\\n",
      "0            0.293855               0.122073             0.559154   \n",
      "1            0.340128               0.127321            -0.202618   \n",
      "2           -0.250742              -0.153541            -0.033103   \n",
      "3            0.069483               0.071301             0.023233   \n",
      "4            0.147199               0.239102             0.016921   \n",
      "\n",
      "   LotArea_x_Fireplaces  ...  OverallQual^1  OverallQual^2  OverallQual^3  \\\n",
      "0              0.488719  ...      -0.099842       0.009968      -0.000995   \n",
      "1              0.616083  ...       0.632038       0.399472       0.252481   \n",
      "2             -0.238921  ...      -0.099842       0.009968      -0.000995   \n",
      "3              0.022165  ...       2.095798       4.392371       9.205523   \n",
      "4             -0.086983  ...       0.632038       0.399472       0.252481   \n",
      "\n",
      "   LotArea^1  LotArea^2  LotArea^3  LotArea^4  2ndFlrSF^1  2ndFlrSF^2  \\\n",
      "0  -0.491264   0.241340  -0.118562   0.058245   -0.808132    0.653077   \n",
      "1  -0.619291   0.383521  -0.237511   0.147088    0.849426    0.721525   \n",
      "2   0.240165   0.057679   0.013852   0.003327    0.722619    0.522178   \n",
      "3   0.037695   0.001421   0.000054   0.000002   -0.808132    0.653077   \n",
      "4  -0.147924   0.021881  -0.003237   0.000479    0.709032    0.502727   \n",
      "\n",
      "   2ndFlrSF^3  \n",
      "0   -0.527772  \n",
      "1    0.612883  \n",
      "2    0.377335  \n",
      "3   -0.527772  \n",
      "4    0.356449  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Index(['OverallCond', 'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'GarageArea',\n",
      "       'Fireplaces', 'LotArea_x_1stFlrSF', 'LotArea_x_TotalBsmtSF',\n",
      "       'LotArea_x_GrLivArea', 'LotArea_x_Fireplaces',\n",
      "       '2ndFlrSF_x_TotRmsAbvGrd', 'OverallCond_x_TotalBsmtSF',\n",
      "       'OverallQual_x_2ndFlrSF', 'GrLivArea^1', 'GrLivArea^2', 'GrLivArea^3',\n",
      "       'OverallQual^1', 'OverallQual^2', 'OverallQual^3', 'LotArea^1',\n",
      "       'LotArea^2', 'LotArea^3', 'LotArea^4', '2ndFlrSF^1', '2ndFlrSF^2',\n",
      "       '2ndFlrSF^3'],\n",
      "      dtype='object')\n",
      "Index(['OverallCond', 'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'GarageArea',\n",
      "       'Fireplaces', 'LotArea_x_1stFlrSF', 'LotArea_x_TotalBsmtSF',\n",
      "       'LotArea_x_GrLivArea', 'LotArea_x_Fireplaces',\n",
      "       '2ndFlrSF_x_TotRmsAbvGrd', 'OverallCond_x_TotalBsmtSF',\n",
      "       'OverallQual_x_2ndFlrSF', 'GrLivArea^1', 'GrLivArea^2', 'GrLivArea^3',\n",
      "       'OverallQual^1', 'OverallQual^2', 'OverallQual^3', 'LotArea^1',\n",
      "       'LotArea^2', 'LotArea^3', 'LotArea^4', '2ndFlrSF^1', '2ndFlrSF^2',\n",
      "       '2ndFlrSF^3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Display the first few rows of the updated X_train_scaled to verify the new columns\n",
    "print(X_train_scaled.head())\n",
    "\n",
    "# Display the first few rows of the updated X_val_scaled to verify the new columns\n",
    "print(X_val_scaled.head())\n",
    "\n",
    "# Display the columns to ensure interaction and polynomial terms are included\n",
    "print(X_train_scaled.columns)\n",
    "print(X_val_scaled.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model R-Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the $R^2$ of the full model with your interaction and polynomial terms added. Print this value for both the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.8526309398926483\n",
      "Validation R²: 0.7516461451222044\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Fit a linear regression model on the updated training data\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate the R² score on the training data\n",
    "train_r2 = linreg.score(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate the R² score on the validation data\n",
    "val_r2 = linreg.score(X_val_scaled, y_val)\n",
    "\n",
    "# Print the R² scores\n",
    "print(f\"Train R²: {train_r2}\")\n",
    "print(f\"Validation R²: {val_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we may be overfitting some now. Let's try some feature selection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "First, test out `RFE` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)) with several different `n_features_to_select` values. For each value, print out the train and validation $R^2$ score and how many features remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features_to_select: 5\n",
      "Train R²: 0.776039994126505\n",
      "Validation R²: 0.635298172527236\n",
      "Number of features selected: 5\n",
      "------------------------------\n",
      "n_features_to_select: 10\n",
      "Train R²: 0.8235682746440406\n",
      "Validation R²: 0.7699939539204792\n",
      "Number of features selected: 10\n",
      "------------------------------\n",
      "n_features_to_select: 15\n",
      "Train R²: 0.8358457626644923\n",
      "Validation R²: 0.8369035748713005\n",
      "Number of features selected: 15\n",
      "------------------------------\n",
      "n_features_to_select: 20\n",
      "Train R²: 0.8494610550826857\n",
      "Validation R²: 0.7996898379311587\n",
      "Number of features selected: 20\n",
      "------------------------------\n",
      "n_features_to_select: 25\n",
      "Train R²: 0.852630931786299\n",
      "Validation R²: 0.7517809760135477\n",
      "Number of features selected: 25\n",
      "------------------------------\n",
      "n_features_to_select: 30\n",
      "Train R²: 0.8526309398926483\n",
      "Validation R²: 0.7516461451222044\n",
      "Number of features selected: 30\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Define a range of n_features_to_select values\n",
    "n_features_range = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "# Loop over the range of n_features_to_select values\n",
    "for n_features in n_features_range:\n",
    "    # Initialize RFE with a linear regression estimator\n",
    "    rfe = RFE(estimator=LinearRegression(), n_features_to_select=n_features)\n",
    "    \n",
    "    # Fit RFE on the training data\n",
    "    rfe.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Transform the training and validation datasets\n",
    "    X_train_rfe = rfe.transform(X_train_scaled)\n",
    "    X_val_rfe = rfe.transform(X_val_scaled)\n",
    "    \n",
    "    # Fit a linear regression model on the transformed training data\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train_rfe, y_train)\n",
    "    \n",
    "    # Calculate the R² score on the training data\n",
    "    train_r2 = linreg.score(X_train_rfe, y_train)\n",
    "    \n",
    "    # Calculate the R² score on the validation data\n",
    "    val_r2 = linreg.score(X_val_rfe, y_val)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"n_features_to_select: {n_features}\")\n",
    "    print(f\"Train R²: {train_r2}\")\n",
    "    print(f\"Validation R²: {val_r2}\")\n",
    "    print(f\"Number of features selected: {n_features}\")\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Final Model on Test Data\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "At the start of this lab, we created `X_test` and `y_test`. Prepare `X_test` the same way that `X_train` and `X_val` have been prepared. This includes scaling, adding interactions, and adding polynomial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OverallCond  TotalBsmtSF  1stFlrSF  TotRmsAbvGrd  GarageArea  Fireplaces  \\\n",
      "0     2.211546    -0.007794 -0.299094     -0.365525   -1.019917   -0.994820   \n",
      "1    -0.509252     0.954980  0.875424      1.483117    1.120866    2.170867   \n",
      "2     0.397681    -0.129332 -0.407845     -0.981739   -0.561178    0.588023   \n",
      "3     1.304613    -0.138864 -0.473096      0.250689   -0.274466    2.170867   \n",
      "4    -0.509252     1.329127  1.201679     -0.365525    2.076573    0.588023   \n",
      "\n",
      "   LotArea*OverallQual  TotalBsmtSF*GrLivArea  GrLivArea^1  GrLivArea^2  ...  \\\n",
      "0             0.020576               0.007196    -0.923274     0.852435  ...   \n",
      "1             0.147591               2.017851     2.112978     4.464675  ...   \n",
      "2             0.134258               0.129516    -1.001427     1.002856  ...   \n",
      "3             0.048461              -0.033495     0.241209     0.058182  ...   \n",
      "4             0.336860               0.206334     0.155240     0.024100  ...   \n",
      "\n",
      "   OverallQual^1  OverallQual^2  OverallQual^3  LotArea^1  LotArea^2  \\\n",
      "0      -0.099842       0.009968      -0.000995  -0.206088   0.042472   \n",
      "1       1.363918       1.860273       2.537259   0.108211   0.011710   \n",
      "2      -0.831723       0.691762      -0.575354  -0.161422   0.026057   \n",
      "3      -0.099842       0.009968      -0.000995  -0.485374   0.235588   \n",
      "4       2.095798       4.392371       9.205523   0.160731   0.025834   \n",
      "\n",
      "   LotArea^3  LotArea^4  2ndFlrSF^1  2ndFlrSF^2  2ndFlrSF^3  \n",
      "0  -0.008753   0.001804   -0.808132    0.653077   -0.527772  \n",
      "1   0.001267   0.000137    1.732552    3.001735    5.200662  \n",
      "2  -0.004206   0.000679   -0.808132    0.653077   -0.527772  \n",
      "3  -0.114348   0.055502    0.686388    0.471128    0.323377  \n",
      "4   0.004152   0.000667   -0.808132    0.653077   -0.527772  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Step 1: Scale X_test using the same scaler fitted on X_train\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Step 2: Add interaction terms to X_test\n",
    "# Assuming interaction terms were added in a similar manner as in previous steps\n",
    "# Example: Adding interaction terms for pairs of columns\n",
    "interaction_cols = [('LotArea', 'OverallQual'), ('TotalBsmtSF', 'GrLivArea')]  # Example pairs\n",
    "\n",
    "for col1, col2 in interaction_cols:\n",
    "    X_test_scaled[f'{col1}*{col2}'] = X_test_scaled[col1] * X_test_scaled[col2]\n",
    "\n",
    "# Step 3: Add polynomial terms to X_test based on the best polynomial terms identified\n",
    "for col, (degree, r2) in best_polynomials.items():\n",
    "    # Instantiate PolynomialFeatures with relevant degree\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    # Fit polynomial to column and transform column\n",
    "    poly_test = poly.fit_transform(X_test_scaled[[col]])\n",
    "    \n",
    "    # Convert the result to a DataFrame\n",
    "    poly_test_df = pd.DataFrame(poly_test, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "    \n",
    "    # Drop the original column before combining\n",
    "    X_test_scaled = X_test_scaled.drop(columns=[col])\n",
    "    \n",
    "    # Add polynomial to data\n",
    "    X_test_scaled = pd.concat([X_test_scaled, poly_test_df], axis=1)\n",
    "\n",
    "# Display the first few rows of the updated X_test_scaled to verify the new columns\n",
    "print(X_test_scaled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either `RFE` or `Lasso`, fit a model on the complete train + validation set, then find R-Squared and MSE values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LotArea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'LotArea'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add interaction terms to X_test_scaled\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col1, col2 \u001b[38;5;129;01min\u001b[39;00m interaction_cols:\n\u001b[1;32m----> 6\u001b[0m     X_test_scaled[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test_scaled[col1] \u001b[38;5;241m*\u001b[39m X_test_scaled[col2]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Add polynomial terms to X_test based on the best polynomial terms identified\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, (degree, r2) \u001b[38;5;129;01min\u001b[39;00m best_polynomials\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Instantiate PolynomialFeatures with relevant degree\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'LotArea'"
     ]
    }
   ],
   "source": [
    "# Assuming interaction terms were added in a similar manner as in previous steps\n",
    "interaction_cols = [('LotArea', 'OverallQual'), ('TotalBsmtSF', 'GrLivArea')]  # Example pairs\n",
    "\n",
    "# Add interaction terms to X_test_scaled\n",
    "for col1, col2 in interaction_cols:\n",
    "    X_test_scaled[f'{col1}*{col2}'] = X_test_scaled[col1] * X_test_scaled[col2]\n",
    "\n",
    "# Add polynomial terms to X_test based on the best polynomial terms identified\n",
    "for col, (degree, r2) in best_polynomials.items():\n",
    "    # Instantiate PolynomialFeatures with relevant degree\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    # Fit polynomial to column and transform column\n",
    "    poly_test = poly.fit_transform(X_test_scaled[[col]])\n",
    "    \n",
    "    # Convert the result to a DataFrame\n",
    "    poly_test_df = pd.DataFrame(poly_test, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "    \n",
    "    # Drop the original column before combining\n",
    "    X_test_scaled = X_test_scaled.drop(columns=[col])\n",
    "    \n",
    "    # Add polynomial to data\n",
    "    X_test_scaled = pd.concat([X_test_scaled, poly_test_df], axis=1)\n",
    "\n",
    "# Ensure the same columns are present in X_test_scaled as in X_train_val\n",
    "X_test_scaled = X_test_scaled[X_train_val.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- LotArea*OverallQual\n- TotalBsmtSF*GrLivArea\nFeature names seen at fit time, yet now missing:\n- 2ndFlrSF_x_TotRmsAbvGrd\n- LotArea_x_1stFlrSF\n- LotArea_x_Fireplaces\n- LotArea_x_GrLivArea\n- LotArea_x_TotalBsmtSF\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Transform the combined training and validation datasets\u001b[39;00m\n\u001b[0;32m     14\u001b[0m X_train_val_rfe \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mtransform(X_train_val)\n\u001b[1;32m---> 15\u001b[0m X_test_rfe \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mtransform(X_test_scaled)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Fit a linear regression model on the transformed combined training and validation data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m linreg \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:107\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m preserve_X \u001b[38;5;241m=\u001b[39m output_config_dense \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_pandas_df(X)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    108\u001b[0m     X,\n\u001b[0;32m    109\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    110\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    111\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _safe_tags(\u001b[38;5;28mself\u001b[39m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_nan\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    112\u001b[0m     cast_to_ndarray\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m preserve_X,\n\u001b[0;32m    113\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- LotArea*OverallQual\n- TotalBsmtSF*GrLivArea\nFeature names seen at fit time, yet now missing:\n- 2ndFlrSF_x_TotRmsAbvGrd\n- LotArea_x_1stFlrSF\n- LotArea_x_Fireplaces\n- LotArea_x_GrLivArea\n- LotArea_x_TotalBsmtSF\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the number of features to select\n",
    "n_features_to_select = 20  # Example value, adjust as needed\n",
    "\n",
    "# Initialize RFE with a linear regression estimator\n",
    "rfe = RFE(estimator=LinearRegression(), n_features_to_select=n_features_to_select)\n",
    "\n",
    "# Fit RFE on the combined training and validation data\n",
    "rfe.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Transform the combined training and validation datasets\n",
    "X_train_val_rfe = rfe.transform(X_train_val)\n",
    "X_test_rfe = rfe.transform(X_test_scaled)\n",
    "\n",
    "# Fit a linear regression model on the transformed combined training and validation data\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_val_rfe, y_train_val)\n",
    "\n",
    "# Calculate the R² score on the test data\n",
    "test_r2 = linreg.score(X_test_rfe, y_test)\n",
    "\n",
    "# Calculate the MSE on the test data\n",
    "test_mse = mean_squared_error(y_test, linreg.predict(X_test_rfe))\n",
    "\n",
    "# Print the R² and MSE values\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Boolean index has wrong length: 26 instead of 21",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m lasso\u001b[38;5;241m.\u001b[39mfit(X_train_val, y_train_val)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Transform the test dataset using the fitted Lasso model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m X_test_lasso \u001b[38;5;241m=\u001b[39m X_test_scaled\u001b[38;5;241m.\u001b[39mloc[:, lasso\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate the R² score on the test data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m test_r2 \u001b[38;5;241m=\u001b[39m lasso\u001b[38;5;241m.\u001b[39mscore(X_test_scaled, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(retval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\u001b[38;5;241m.\u001b[39m_getitem_axis(key, axis\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1413\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1209\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getbool_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, axis: AxisInt):\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;66;03m# caller is responsible for ensuring non-None axis\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m-> 1209\u001b[0m     key \u001b[38;5;241m=\u001b[39m check_bool_indexer(labels, key)\n\u001b[0;32m   1210\u001b[0m     inds \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(inds, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2681\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_like(result):\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;66;03m# GH 33924\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# key may contain nan elements, check_array_indexer needs bool array\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     result \u001b[38;5;241m=\u001b[39m pd_array(result, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m-> 2681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m check_array_indexer(index, result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:539\u001b[0m, in \u001b[0;36mcheck_array_indexer\u001b[1;34m(array, indexer)\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;66;03m# GH26658\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(array):\n\u001b[1;32m--> 539\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoolean index has wrong length: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(indexer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(array)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         )\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer_dtype(dtype):\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: Boolean index has wrong length: 26 instead of 21"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize Lasso with a chosen alpha value\n",
    "lasso = Lasso(alpha=0.01)  # Example value, adjust as needed\n",
    "\n",
    "# Fit Lasso on the combined training and validation data\n",
    "lasso.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Transform the test dataset using the fitted Lasso model\n",
    "X_test_lasso = X_test_scaled.loc[:, lasso.coef_ != 0]\n",
    "\n",
    "# Calculate the R² score on the test data\n",
    "test_r2 = lasso.score(X_test_scaled, y_test)\n",
    "\n",
    "# Calculate the MSE on the test data\n",
    "test_mse = mean_squared_error(y_test, lasso.predict(X_test_scaled))\n",
    "\n",
    "# Print the R² and MSE values\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up Ideas (Optional)\n",
    "\n",
    "### Create a Lasso Path\n",
    "\n",
    "From this section, you know that when using `Lasso`, more parameters shrink to zero as your regularization parameter goes up. In scikit-learn there is a function `lasso_path()` which visualizes the shrinkage of the coefficients while $alpha$ changes. Try this out yourself!\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py\n",
    "\n",
    "### AIC and BIC for Subset Selection\n",
    "\n",
    "This notebook shows how you can use AIC and BIC purely for feature selection. Try this code out on our Ames housing data!\n",
    "\n",
    "https://xavierbourretsicotte.github.io/subset_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to apply concepts of bias-variance tradeoff using extensions to linear models and feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
